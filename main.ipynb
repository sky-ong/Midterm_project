{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data generation\n",
    "def generate_synthetic_data(num_points=20):\n",
    "    t = np.sort(np.random.uniform(0, 1, num_points))  # Irregular time points\n",
    "    x = np.sin(2 * np.pi * t) + np.random.normal(0, 0.1, num_points)\n",
    "    return t, x\n",
    "\n",
    "# Time embedding\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.w = nn.Linear(1, dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        out = torch.cos(self.w(t))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Time Attention Layer\n",
    "class MultiTimeAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super(MultiTimeAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(input_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(input_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        self.time_embedding = TimeEmbedding(embed_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, L, _ = x.shape\n",
    "        \n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "\n",
    "        te = self.time_embedding(t)\n",
    "\n",
    "        q = q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        te = te.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attn = attn + torch.matmul(q, te.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, self.embed_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved mTAN model\n",
    "class ImprovedMTAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(ImprovedMTAN, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            MultiTimeAttention(hidden_dim, hidden_dim, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        x = self.input_proj(x.unsqueeze(-1))\n",
    "        for layer in self.attention_layers:\n",
    "            x = layer(x, t) + x  # Residual connection\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, t, x, epochs=5000, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5, verbose=True)\n",
    "    t_tensor = torch.tensor(t).float().unsqueeze(0)\n",
    "    x_tensor = torch.tensor(x).float().unsqueeze(0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(t_tensor, x_tensor)\n",
    "        loss = nn.MSELoss()(output, x_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation function\n",
    "def interpolate(model, t, x, t_dense):\n",
    "    with torch.no_grad():\n",
    "        # t_dense에 해당하는 새로운 x 값을 생성합니다.\n",
    "        x_dense = np.interp(t_dense, t, x)  \n",
    "\n",
    "        # 원래 x 값을 사용하는 대신 보간된 x_dense를 사용합니다.\n",
    "        t_dense_tensor = torch.tensor(t_dense).float().unsqueeze(0)\n",
    "        x_dense_tensor = torch.tensor(x_dense).float().unsqueeze(0)\n",
    "        \n",
    "        # model에 t_dense_tensor와 x_dense_tensor를 전달합니다.\n",
    "        output = model(t_dense_tensor, x_dense_tensor)\n",
    "        return output.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main experiment\n",
    "t, x = generate_synthetic_data(num_points=30)\n",
    "\n",
    "mtan_model = ImprovedMTAN(input_dim=1, hidden_dim=64, num_heads=4, num_layers=2)\n",
    "\n",
    "print(\"Training mTAN model:\")\n",
    "train_model(mtan_model, t, x)\n",
    "\n",
    "t_dense = np.linspace(0, 1, 1000)\n",
    "mtan_interp = interpolate(mtan_model, t, x, t_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t_dense, np.sin(2 * np.pi * t_dense), 'r-', label='Ground truth')\n",
    "plt.plot(t, x, 'kx', label='Observed data')\n",
    "plt.plot(t_dense, mtan_interp, 'b-', label='mTAN interpolation')\n",
    "plt.legend()\n",
    "plt.title(\"mTAN Interpolation on Irregular Time Series\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
